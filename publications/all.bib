@inproceedings{qiang21_iconip,
  title={Speech Dereverberation Based on Scale-aware Mean Square Error Loss},
  author={Luya Qiang and Hao Shi (Joint First Author) and Meng Ge and Haoran Yin and Nan Li and Longbiao Wang and Sheng Li and Jianwu Dang},
  year={2021},
  _venue={ICONIP},
  url={https://github.com/hshi-speech/resume/blob/main/pdf/SaSD.pdf},
  selected={true},
  abstract={
  Recently, deep learning-based speech dereverberation approaches have achieved remarkable performance by directly 
  mapping the input spectrogram to a target spectrogram or time-frequency mask. However, these approaches are usually 
  optimized under distance-related objective functions: the mean square error (MSE). The traditional MSE training 
  criterion results in a strong inherent uniform variance statistical assumption on the target speech and noise during 
  training, which cannot be satisfied in real-world scenarios. To alleviate such an assumption mismatch problem, we 
  propose a speech dereverberation solution called Scale-aware Speech Dereverberation (SaSD) based on scaled-MSE. 
  Specifically, we modify the MSE with different scales for each frequency band and progressively reduce the gap 
  between the low- and high-frequency ranges to make the error follow the assumption of MSE assumption. Experiments 
  demonstrated that SaSD achieved 1.0 SRMR and 0.8 PESQ improvements over the mapping baseline system.  
  }
}

@inproceedings{yin21_iconip,
  title={Simultaneous Progressive Filtering-based Monaural Speech Enhancement},
  author={Haoran Yin and Hao Shi (Joint First Author) and Longbiao Wang and Luya Qiang and Sheng Li and Meng Ge and Gaoyan Zhang and Jianwu Dang},
  year={2021},
  _venue={ICONIP},
  url={https://github.com/hshi-speech/resume/blob/main/pdf/iconip2021-yin.pdf},
  selected={true},
  abstract={
  Speech enhancement (SE) benefits from multi-stage stacking. However, this will introduce a lot of new 
  parameters to the neural network. In this paper, we propose a simultaneous progressive filtering based 
  monaural SE model. Mapping-based and masking-based SE systems are simultaneously obtained with 
  multi-target learning (MTL). Different from other MTL systems, our proposed model addresses different 
  enhancement needs. The mapping-based SE system aims to recover speech signals from noisy features. While 
  the masking-based SE system serves as a post-filtering to further reduce the noise that still exists 
  after the mapping-based SE system. With the high signal-to-noise ratio inputs, noise reduction of the 
  masking-based SE system is obvious with little speech signal loss. These two stages share one neural 
  network which controls the parameters of the entire system with little or no increase. In addition, 
  our approach is easy to integrate with existing methods and improve their performance significantly and 
  stably. The experiments on Valentini-Botinhao data set show our proposed model achieves 0.12 PESQ 
  improvement compared with directly mapping-based and masking-based SE systems both in single-target 
  and multi-target learning. Furthermore, by comparing spectrograms, we find that our proposed models are 
  able to recover better harmonic information.
  }
}

@inproceedings{shi21_apsipa,
  title={Spectrograms Fusion-based End-to-end Robust Automatic Speech Recognition},
  author={Hao Shi and Longbiao Wang and Sheng Li and Cunhang Fan and Jianwu Dang and Tatsuya Kawahara},
  year={2021},
  _venue={APSIPA},
  url={https://github.com/hshi-speech/resume/blob/main/pdf/APSIPA-2021.pdf},
  selected={true},
  abstract={
  To improve the robustness of automatic speech recognition (ASR), speech enhancement (SE) is often used 
  as a front-end noise-removal process. Although there is complementarity between the mapping-based and 
  the mask-based SE system, one of the SE systems has been conventionally used as the front-end of ASR. 
  We propose a spectrogram fusion (SF)-based end-to-end (E2E) robust ASR system, in which the mapping-based 
  and masking-based SE are used as the front-end simultaneously. We adopt SF to combine the advantages of 
  mapping-based and masking-based SE systems. SF and ASR modules are connected in an E2E manner, and joint 
  training is conducted to finetune the front-end and the back-end. We compared the performance of 
  different front-ends after joint training. From the experiments using Aishell and PNL 100 Nonspeech 
  Sounds datasets, we found that the fusion of two SEs are beneficial for ASR, especially under low 
  signal-to-noise ratio, where a relative improvement of more than 7% is achieved.  
  }
}


@inproceedings{shi20_interspeech,
  title={Singing Voice Extraction with Attention-Based Spectrograms Fusion},
  author={Hao Shi and Longbiao Wang and Sheng Li and Chenchen Ding and Meng Ge and Nan Li and Jianwu Dang and Hiroshi Seki},
  year={2020},
  _venue={INTERSPEECH},
  url={https://github.com/hshi-speech/resume/blob/main/pdf/Wed-1-11-1.pdf},
  _talkurl={https://hshi-speech.github.io/resume/videos/1043_paper_Hao%20Shi_Singing%20Voice%20Extraction%20with%20Attention%20based%20Spectrograms%20Fusion.mp4},
  selected={true},
  abstract={
  We propose a novel attention mechanism-based spectrograms fusion system with minimum difference masks (MDMs) 
  estimation for singing voice extraction. Compared with previous works that use a fully connected neural 
  network, our system takes advantage of the multi-head attention mechanism. Specifically, we 1) try a variety 
  of embedding methods of multiple spectrograms as the input of attention mechanisms, which can provide 
  multi-scale correlation information between adjacent frames in the spectrograms; 2) add a regular term to 
  loss function to obtain better continuity of spectrogram; 3) use the phase of the linear fusion waveform to 
  reconstruct the final waveform, which can reduce the impact of the inconsistent spectrogram. Experiments on 
  the MIR-1K dataset show that our system consistently improves the quantitative evaluation by the perceptual 
  evaluation of speech quality, signal-to-distortion ratio, signal-to-interference ratio, and signal-to-artifact ratio.
  }
}

@inproceedings{9054661,
  title={Spectrograms Fusion with Minimum Difference Masks Estimation for Monaural Speech Dereverberation},
  author={Hao Shi and Longbiao Wang and Meng Ge and Sheng Li and Jianwu Dang},
  year={2020},
  _venue={ICASSP},
  url={https://github.com/hshi-speech/resume/blob/main/pdf/0007539.pdf},
  _talkurl={https://hshi-speech.github.io/resume/videos/ICASSP2020-3378-SPECTROGRAMS%20FUSION%20WITH%20MINIMUM%20DIFFERENCE%20MASKS%20ESTIMATION%20FOR%20MONAURAL%20SPEECH%20DEREVERBERATION-Hao%20Shi.mp4},
  selected={true},
  abstract={
  Spectrograms fusion is an effective method for incorporating complementary speech dereverberation systems. 
  Previous linear spectrograms fusion by averaging multiple spectrograms shows outstanding performance. 
  However, various systems with different features cannot apply this simple method. In this study, we design 
  the minimum difference masks (MDMs) to classify the time-frequency (T-F) bins in spectrograms according to 
  the nearest distances from labels. Then, we propose a two-stage nonlinear spectrograms fusion system for 
  speech dereverberation. First, we conduct a multitarget learning-based speech dereverberation front-end 
  model to get spectrograms simultaneously. Then, MDMs are estimated to take the best parts of different 
  spectrograms. We are using spectrograms in the first stage and MDMs in the second stage to recombine T-F 
  bins. The experiments on the REVERB challenge show that a strong feature complementarity between 
  spectrograms and MDMs. Moreover, the proposed framework can consistently and significantly improve PESQ 
  and SRMR, both real and simulated data, e.g., an average PESQ gain of 0.1 in all simulated data and an 
  average SRMR gain of 1.22 in all real data.
  }
}

@inproceedings{ge19_interspeech,
  title={{Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement}},
  author={Meng Ge and Longbiao Wang and Nan Li and Hao Shi and Jianwu Dang and Xiangang Li},
  year={2019},
  _venue={INTERSPEECH},
  url={https://github.com/hshi-speech/resume/blob/main/pdf/1477.pdf},
  abstract={
  Speech enhancement aims to keep the real speech signal and reduce noise for building robust communication systems. 
  Under the success of DNN, significant progress has been made. Nevertheless, accuracy of the speech enhancement 
  system is not satisfactory due to insufficient consideration of varied environmental and contextual information in 
  complex cases. To address these problems, this research proposes an end-to-end environment-dependent attention-driven 
  approach. The local frequency-temporal pattern via convolutional neural network is fully employed without pooling 
  operation. It then integrates an attention mechanism into bidirectional long short-term memory to acquire the weighted 
  dynamic context between consecutive frames. Furthermore, dynamic environment estimation and phase correction further 
  improve the generalization ability. Extensive experimental results on REVERB challenge demonstrated that the proposed 
  approach outperformed existing methods, improving PESQ from 2.56 to 2.87 and SRMR from 4.95 to 5.50 compared with conventional DNN.
  }
}
